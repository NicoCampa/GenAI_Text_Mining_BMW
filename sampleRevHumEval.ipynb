{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bmw_app_analysis/results/bmw_reviews_consolidated_20250504_155236.csv...\n",
      "Filtering reviews with at least 5 words...\n",
      "Reviews with 5+ words: 13312 out of 18350\n",
      "\n",
      "Reviews available after filtering by language and length:\n",
      "  German: 3839 reviews\n",
      "  Italian: 920 reviews\n",
      "  Spanish: 670 reviews\n",
      "  French: 1155 reviews\n",
      "Sampled 30 reviews from German\n",
      "Sampled 30 reviews from Italian\n",
      "Sampled 30 reviews from Spanish\n",
      "Sampled 30 reviews from French\n",
      "Saved sampled reviews to bmw_app_analysis/results/bmw_reviews_sampledHE.csv\n",
      "\n",
      "Final sample distribution:\n",
      "  German: 30 reviews\n",
      "  Italian: 30 reviews\n",
      "  Spanish: 30 reviews\n",
      "  French: 30 reviews\n",
      "\n",
      "Example reviews from each language:\n",
      "\n",
      "German example:\n",
      "  Original: Die App kann schon viel, aber warum lassen sich di...\n",
      "  English:  The app can already do a lot, but why can't the wi...\n",
      "\n",
      "Italian example:\n",
      "  Original: Dalla versione 2.11.0, con IDrive 6 e telefono And...\n",
      "  English:  Since version 2.11.0, with iDrive 6 and an Android...\n",
      "\n",
      "Spanish example:\n",
      "  Original: Aplicación penosa, en esta nueva actualización se ...\n",
      "  English:  Miserable app, with this new update they’ve even l...\n",
      "\n",
      "French example:\n",
      "  Original: Application très conviviale et informative ... C'e...\n",
      "  English:  Very user-friendly and informative application... ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def sample_reviews_by_language(input_file='bmw_app_analysis/results/bmw_reviews_consolidated_20250504_155236.csv', \n",
    "                              output_file='bmw_app_analysis/results/bmw_reviews_sampledHE.csv'):\n",
    "    \"\"\"\n",
    "    Sample 30 reviews EACH from German, Italian, Spanish and French languages\n",
    "    with at least 5 words from the BMW reviews dataset.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input CSV file\n",
    "        output_file: Path to save the sampled reviews\n",
    "    \"\"\"\n",
    "    # Read the consolidated file\n",
    "    print(f\"Reading {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Verify columns exist\n",
    "    required_columns = ['reviewId', 'content', 'language', 'content_english']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"ERROR: Missing required columns: {missing_columns}\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Apply minimum word filter - INCREASED TO 5 WORDS\n",
    "    min_words = 5  # Changed from 3 to 5\n",
    "    print(f\"Filtering reviews with at least {min_words} words...\")\n",
    "    df['word_count'] = df['content'].apply(lambda x: len(re.findall(r'\\b\\w+\\b', str(x))) if isinstance(x, str) else 0)\n",
    "    df_filtered_by_length = df[df['word_count'] >= min_words].copy()\n",
    "    \n",
    "    print(f\"Reviews with {min_words}+ words: {len(df_filtered_by_length)} out of {len(df)}\")\n",
    "    \n",
    "    # Target languages - in desired order\n",
    "    target_languages = [\"German\", \"Italian\", \"Spanish\", \"French\"]\n",
    "    \n",
    "    # Apply language filter\n",
    "    df_filtered = df_filtered_by_length[df_filtered_by_length['language'].isin(target_languages)].copy()\n",
    "    \n",
    "    # Report on available reviews per language\n",
    "    print(\"\\nReviews available after filtering by language and length:\")\n",
    "    for lang in target_languages:\n",
    "        count = len(df_filtered[df_filtered['language'] == lang])\n",
    "        print(f\"  {lang}: {count} reviews\")\n",
    "    \n",
    "    # Sample 30 from each language group (or all available if less than 30)\n",
    "    sampled_reviews = []\n",
    "    target_per_language = 30  # 30 reviews per language\n",
    "    \n",
    "    for lang in target_languages:\n",
    "        lang_df = df_filtered[df_filtered['language'] == lang]\n",
    "        \n",
    "        if len(lang_df) == 0:\n",
    "            print(f\"No reviews found for {lang}\")\n",
    "            continue\n",
    "            \n",
    "        # Sample up to target_per_language reviews, or all if fewer available\n",
    "        sample_size = min(target_per_language, len(lang_df))\n",
    "        sampled = lang_df.sample(sample_size, random_state=42)\n",
    "        sampled_reviews.append(sampled)\n",
    "        print(f\"Sampled {len(sampled)} reviews from {lang}\")\n",
    "    \n",
    "    # Combine all samples\n",
    "    sampled_df = pd.concat(sampled_reviews, ignore_index=True)\n",
    "    \n",
    "    # Create language order mapping for sorting\n",
    "    lang_order = {lang: i for i, lang in enumerate(target_languages)}\n",
    "    \n",
    "    # Select only the required columns and rename content_english to english_content\n",
    "    output_df = sampled_df[['reviewId', 'content', 'language', 'content_english']].copy()\n",
    "    \n",
    "    # Clean up english_content: remove quotes\n",
    "    output_df['content_english'] = output_df['content_english'].astype(str).str.replace('\"', '')\n",
    "    \n",
    "    # Rename the column\n",
    "    output_df = output_df.rename(columns={'content_english': 'english_content'})\n",
    "    \n",
    "    # Sort by language in the specified order\n",
    "    output_df['lang_order'] = output_df['language'].map(lang_order)\n",
    "    output_df = output_df.sort_values('lang_order').drop('lang_order', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved sampled reviews to {output_file}\")\n",
    "    \n",
    "    print(\"\\nFinal sample distribution:\")\n",
    "    for lang in target_languages:\n",
    "        count = len(output_df[output_df['language'] == lang])\n",
    "        print(f\"  {lang}: {count} reviews\")\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    sampled_reviews = sample_reviews_by_language()\n",
    "    \n",
    "    # Display summary\n",
    "    if sampled_reviews is not None and not sampled_reviews.empty:\n",
    "        # Show a couple examples from each language\n",
    "        print(\"\\nExample reviews from each language:\")\n",
    "        for lang in [\"German\", \"Italian\", \"Spanish\", \"French\"]:\n",
    "            lang_samples = sampled_reviews[sampled_reviews['language'] == lang].head(1)\n",
    "            if not lang_samples.empty:\n",
    "                print(f\"\\n{lang} example:\")\n",
    "                for _, row in lang_samples.iterrows():\n",
    "                    print(f\"  Original: {row['content'][:50]}{'...' if len(row['content']) > 50 else ''}\")\n",
    "                    print(f\"  English:  {row['english_content'][:50]}{'...' if len(row['english_content']) > 50 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bmw_topic_cards.py  –  v6  (2025‑05‑02)\n",
    "# ================================================================\n",
    "# • Stratified random sampling (mirrors topic sentiment mix)\n",
    "# • One paragraph summary  +  “‑ ” negative bullets  +  “+ ” positive bullets\n",
    "# • Prompts stored inside TopicSummary.prompts  (audit / debugging)\n",
    "# • No JSON parsing, so Ollama chatter can’t break the pipeline\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import html\n",
    "import logging\n",
    "import subprocess\n",
    "import textwrap\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  CONFIGURATION\n",
    "# -------------------------------------------------------------------\n",
    "TOP_N_TOPICS        = 15          # how many topics to visualise\n",
    "PROMPT_SAMPLE_SIZE  = 300         # reviews fed to each LLM prompt\n",
    "N_KEYWORDS          = 8\n",
    "N_PHRASES           = 5\n",
    "MAX_BULLETS         = 7           # issues / positives\n",
    "BMW_BLUE            = \"#0066B1\"\n",
    "\n",
    "STAR_SVG = (\n",
    "    \"<svg width='14' height='14' viewBox='0 0 24 24' \"\n",
    "    \"xmlns='http://www.w3.org/2000/svg' style='vertical-align:-2px'>\"\n",
    "    \"<polygon fill='#FFD700' \"\n",
    "    \"points='12 2 15.09 8.26 22 9.27 17 14.14 \"\n",
    "    \"18.18 21.02 12 17.77 5.82 21.02 7 14.14 2 9.27 8.91 8.26'/></svg>\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  NLTK – bootstrap stop‑words\n",
    "# -------------------------------------------------------------------\n",
    "for res in (\"stopwords\", \"punkt\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{res}\")\n",
    "    except LookupError:\n",
    "        nltk.download(res, quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\")) | {\n",
    "    \"bmw\", \"app\", \"car\", \"please\", \"would\", \"also\", \"get\", \"use\", \"using\"\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  OLLAMA RUNNER  (temperature 0 for determinism)\n",
    "# -------------------------------------------------------------------\n",
    "def _subprocess_runner(prompt: str, model: str) -> str:\n",
    "    proc = subprocess.run(\n",
    "        [\"ollama\", \"run\", model, \"-t\", \"0\"],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        check=False,\n",
    "    )\n",
    "    return proc.stdout\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  STRATIFIED SAMPLING  (mirror sentiment mix)\n",
    "# -------------------------------------------------------------------\n",
    "def _stratified_sample(df: pd.DataFrame, size: int, seed: int = 0) -> List[str]:\n",
    "    dist = df[\"sentiment\"].value_counts(normalize=True)\n",
    "    quota = {s: int(round(dist.get(s, 0) * size)) for s in (\"negative\", \"positive\", \"neutral\")}\n",
    "    diff = size - sum(quota.values())\n",
    "    if diff:\n",
    "        quota[max(dist, key=dist.get, default=\"negative\")] += diff\n",
    "\n",
    "    col = \"content_english\" if \"content_english\" in df.columns else \"content\"\n",
    "    texts: List[str] = []\n",
    "\n",
    "    for sentiment, n in quota.items():\n",
    "        if n <= 0:\n",
    "            continue\n",
    "        bucket = df[df[\"sentiment\"] == sentiment]\n",
    "        sample_n = min(n, len(bucket))\n",
    "        texts.extend(bucket.sample(sample_n, random_state=seed)[col].tolist())\n",
    "\n",
    "    # top‑up if any bucket ran short\n",
    "    if len(texts) < size:\n",
    "        short = size - len(texts)\n",
    "        remainder = df.drop(df.index[df[col].isin(texts)]).sample(short, random_state=seed)\n",
    "        texts.extend(remainder[col].tolist())\n",
    "\n",
    "    return texts\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  BULLET‑PARSER\n",
    "# -------------------------------------------------------------------\n",
    "def _parse_bullets(text: str, prefix: str) -> List[str]:\n",
    "    \"\"\"Return list of lines starting with the prefix (‘- ’ or ‘+ ’).\"\"\"\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    bullets = [ln[len(prefix):].strip() for ln in lines if ln.startswith(prefix)]\n",
    "    # fallback to comma‑sep if model ignored prefixes\n",
    "    if not bullets and \",\" in text:\n",
    "        bullets = [part.strip() for part in text.split(\",\") if part.strip()]\n",
    "    return bullets[:MAX_BULLETS]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  DATACLASS\n",
    "# -------------------------------------------------------------------\n",
    "@dataclass\n",
    "class TopicSummary:\n",
    "    summary: str\n",
    "    issues: List[str]\n",
    "    positives: List[str]\n",
    "    avg_rating: float\n",
    "    review_count: int\n",
    "    sentiment_dist: Dict[str, float]\n",
    "    top_keywords: List[str]\n",
    "    top_phrases: List[str]\n",
    "    prompts: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def stars(self) -> str:\n",
    "        full, half = int(self.avg_rating), self.avg_rating - int(self.avg_rating) >= 0.5\n",
    "        return STAR_SVG * full + (STAR_SVG if half else \"\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  MAIN ENTRY\n",
    "# -------------------------------------------------------------------\n",
    "def create_topic_cards_from_classified(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    ollama_model_name: str = \"gemma3:12b\",\n",
    "    ollama_runner: Callable[[str, str], str] = _subprocess_runner,\n",
    ") -> Dict[str, TopicSummary]:\n",
    "    \"\"\"\n",
    "    Render topic cards & a rating chart; return dict[topic] -> TopicSummary.\n",
    "    Prompts are stored in `TopicSummary.prompts`.\n",
    "    \"\"\"\n",
    "    log = _setup_logger()\n",
    "\n",
    "    # -------- sanity checks -----------------------------------------\n",
    "    if {\"topics\", \"sentiment\"} - set(df.columns):\n",
    "        raise ValueError(\"DataFrame must include 'topics' & 'sentiment'.\")\n",
    "\n",
    "    text_col = \"content_english\" if \"content_english\" in df.columns else \"content\"\n",
    "\n",
    "    if \"score\" not in df.columns:\n",
    "        df[\"score\"] = (\n",
    "            df[\"sentiment\"]\n",
    "            .map({\"positive\": 4.5, \"neutral\": 3.0, \"negative\": 1.5})\n",
    "            .fillna(3.0)\n",
    "        )\n",
    "\n",
    "    # -------- pick top topics --------------------------------------\n",
    "    topics_freq = df[\"topics\"].str.get_dummies(sep=\",\").sum()\n",
    "    top_topics = (\n",
    "        topics_freq.sort_values(ascending=False).head(TOP_N_TOPICS).index\n",
    "    )\n",
    "\n",
    "    results: Dict[str, TopicSummary] = {}\n",
    "\n",
    "    for topic in tqdm(top_topics, desc=\"Topics\"):\n",
    "        sub = df[df[\"topics\"].str.contains(fr\"\\b{topic}\\b\", case=False, na=False)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        avg = sub[\"score\"].mean()\n",
    "        mix = sub[\"sentiment\"].value_counts(normalize=True).to_dict()\n",
    "\n",
    "        tokens = word_tokenize(\" \".join(sub[text_col].fillna(\"\").str.lower()))\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in STOPWORDS and len(t) > 2]\n",
    "        keywords = [w for w, _ in Counter(tokens).most_common(N_KEYWORDS)]\n",
    "        bigrams = [f\"{a} {b}\" for a, b in zip(tokens, tokens[1:])]\n",
    "        phrases = [p for p, _ in Counter(bigrams).most_common(N_PHRASES)]\n",
    "\n",
    "        sample_n = min(PROMPT_SAMPLE_SIZE, len(sub))\n",
    "        sample_texts = _stratified_sample(sub, sample_n)\n",
    "        sample_blob = \"\\n\".join(f\"[{i}] {txt}\" for i, txt in enumerate(sample_texts))\n",
    "\n",
    "        ctx_block = (\n",
    "            f\"Average score {avg:.2f} (n={len(sub)}). \"\n",
    "            f\"Sentiment mix: neg {mix.get('negative',0):.0%}, \"\n",
    "            f\"pos {mix.get('positive',0):.0%}, \"\n",
    "            f\"neu {mix.get('neutral',0):.0%}.\\n\"\n",
    "            f\"Top keywords: {', '.join(keywords)}.\\n\"\n",
    "            f\"Top phrases: {', '.join(phrases)}.\"\n",
    "        )\n",
    "\n",
    "        prompts: Dict[str, str] = {}\n",
    "\n",
    "        def build(kind: str) -> str:\n",
    "            \"\"\"Return fully‑formed prompt & store it\"\"\"\n",
    "            header = (\n",
    "                \"You are an analytical assistant. Follow ALL rules:\\n\"\n",
    "                f\"• Focus **only** on the topic '{topic}'.\\n\"\n",
    "                \"• Ignore unrelated features.\\n\"\n",
    "                \"• Use English even if reviews were translated.\\n\"\n",
    "                \"• Be factual, concise (temperature 0).\\n\"\n",
    "            )\n",
    "            if kind == \"summary\":\n",
    "                body = (\n",
    "                    \"Write one concise paragraph (3‑5 sentences) that \"\n",
    "                    \"summarises what users say about this topic, covering \"\n",
    "                    \"both pain points and praise.\"\n",
    "                )\n",
    "            else:\n",
    "                label = \"negative issues\" if kind == \"issues\" else \"positive aspects\"\n",
    "                prefix = \"-\" if kind == \"issues\" else \"+\"\n",
    "                body = (\n",
    "                    f\"List up to {MAX_BULLETS} main {label}. \"\n",
    "                    f\"Each line MUST start with '{prefix} '. \"\n",
    "                    \"Use short noun phrases, no duplication, no periods.\"\n",
    "                )\n",
    "\n",
    "            prompt = (\n",
    "                f\"{header}\\n\\nCONTEXT:\\n{ctx_block}\\n\\nSAMPLES:\\n{sample_blob}\\n\\nTASK:\\n{body}\"\n",
    "            )\n",
    "            prompts[kind] = prompt\n",
    "            return prompt\n",
    "\n",
    "        # ----- call Ollama (three prompts in parallel) ----------------\n",
    "        with ThreadPoolExecutor(max_workers=3) as pool:\n",
    "            futures = {\n",
    "                pool.submit(ollama_runner, build(k), ollama_model_name): k\n",
    "                for k in (\"summary\", \"issues\", \"positives\")\n",
    "            }\n",
    "            raw = {k: f.result().strip() for f, k in futures.items()}\n",
    "\n",
    "        summary_text = raw[\"summary\"]\n",
    "        issues_list  = _parse_bullets(raw[\"issues\"], \"-\")\n",
    "        pos_list     = _parse_bullets(raw[\"positives\"], \"+\")\n",
    "\n",
    "        results[topic] = TopicSummary(\n",
    "            summary=summary_text,\n",
    "            issues=issues_list,\n",
    "            positives=pos_list,\n",
    "            avg_rating=avg,\n",
    "            review_count=len(sub),\n",
    "            sentiment_dist=mix,\n",
    "            top_keywords=keywords,\n",
    "            top_phrases=phrases,\n",
    "            prompts=prompts,\n",
    "        )\n",
    "\n",
    "    # ------------- render & plot -------------------------------------\n",
    "    _render_cards(results)\n",
    "    _plot_chart(results, df)\n",
    "    return {k: v.__dict__ for k, v in results.items()}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  VISUAL HELPER – HTML CARDS\n",
    "# -------------------------------------------------------------------\n",
    "def _render_cards(data: Dict[str, TopicSummary]) -> None:\n",
    "    css = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "          :root {{--blue:{blue};--neg:#E53935;--pos:#43A047;--neu:#FFA000;\n",
    "                 --bg:#fff;--text:#1a1a1a;}}\n",
    "          @media (prefers-color-scheme: dark) {{\n",
    "              :root {{--bg:#121212;--text:#e0e0e0;}}\n",
    "          }}\n",
    "          body {{background:var(--bg);color:var(--text);}}\n",
    "\n",
    "/* container */\n",
    "          .cards {{max-width:1200px;margin:0 auto;font-family:'Helvetica Neue',Arial,sans-serif;}}\n",
    "\n",
    "/* card */\n",
    "          .card {{\n",
    "              display:grid;grid-template-columns:220px 1fr;\n",
    "              border:1px solid #bbb;border-radius:8px;margin:18px 0;\n",
    "              overflow:hidden;box-shadow:0 4px 10px rgba(0,0,0,.12);\n",
    "              animation:fadeIn .4s ease;\n",
    "          }}\n",
    "          @keyframes fadeIn {{from {{opacity:0;transform:translateY(8px);}}\n",
    "                              to   {{opacity:1;transform:translateY(0);}}}}\n",
    "          header {{\n",
    "              grid-column:1/-1;background:linear-gradient(135deg,var(--blue) 0%,#0088cc 100%);\n",
    "              color:#fff;padding:12px 18px;font-size:20px;font-weight:600;\n",
    "          }}\n",
    "\n",
    "/* sidebar */\n",
    "          .side {{\n",
    "              background:#f5f7fa;border-right:1px solid #ddd;\n",
    "              padding:14px;display:flex;flex-direction:column;gap:14px;\n",
    "          }}\n",
    "          .stat .num {{font-weight:700;font-size:19px;line-height:1;color:var(--text);}}\n",
    "          .stat .label{{font-size:11px;text-transform:uppercase;font-weight:600;\n",
    "                       letter-spacing:.4px;color:var(--text);}}\n",
    "          .bar {{display:flex;height:10px;border-radius:4px;overflow:hidden;margin-top:4px;}}\n",
    "          .seg {{flex-shrink:0;transition:opacity .2s;}} .seg:hover {{opacity:.75;}}\n",
    "          .neg{{background:var(--neg);}} .pos{{background:var(--pos);}} .neu{{background:var(--neu);}}\n",
    "\n",
    "/* main */\n",
    "          .main {{padding:18px;line-height:1.6;}}\n",
    "          .summary {{\n",
    "              background:rgba(0,102,177,.07);border-left:3px solid var(--blue);\n",
    "              padding:10px;margin-bottom:16px;line-height:1.6;\n",
    "          }}\n",
    "          .cols {{display:flex;flex-wrap:wrap;gap:24px;margin-bottom:16px;}}\n",
    "          h4 {{margin:0 0 6px;color:var(--blue);font-weight:600;}}\n",
    "          ul {{margin:0;padding-left:18px;}}\n",
    "          li {{margin:4px 0;}}\n",
    "\n",
    "/* chips */\n",
    "          .chips {{margin-bottom:10px;}}\n",
    "          .chip {{\n",
    "              display:inline-block;padding:4px 10px;border-radius:20px;\n",
    "              font-size:11px;font-weight:700;margin:2px;cursor:pointer;\n",
    "              transition:opacity .2s;\n",
    "          }}\n",
    "          .chip:hover {{opacity:.8;}}\n",
    "          .kw{{background:#e3f2fd;color:var(--blue);}}\n",
    "          .ph{{background:#e8f5e9;color:#2e7d32;border:1px solid #c8e6c9;}}\n",
    "\n",
    "/* responsive */\n",
    "          @media (max-width:768px){{\n",
    "              .card{{grid-template-columns:1fr}}\n",
    "              .side{{order:2;border:none;border-top:1px solid #ddd;\n",
    "                    flex-direction:row;justify-content:space-around}}\n",
    "          }}\n",
    "        </style>\n",
    "        \"\"\"\n",
    "    ).format(blue=BMW_BLUE)\n",
    "\n",
    "    output: List[str] = [css, \"<div class='cards'>\"]\n",
    "\n",
    "    # sort by review count\n",
    "    for topic, info in sorted(data.items(), key=lambda x: x[1].review_count, reverse=True):\n",
    "        if topic.lower() == \"other\":\n",
    "            continue\n",
    "\n",
    "        bar = \"\".join(_bar_seg(name, frac) for name, frac in info.sentiment_dist.items())\n",
    "        kw_html = \"\".join(f\"<span class='chip kw'>{html.escape(k)}</span>\" for k in info.top_keywords)\n",
    "        ph_html = \"\".join(f\"<span class='chip ph'>{html.escape(p)}</span>\" for p in info.top_phrases)\n",
    "        issues_html = \"\".join(f\"<li>{html.escape(i)}</li>\" for i in info.issues) or \"<li>No major issues</li>\"\n",
    "        pos_html = \"\".join(f\"<li>{html.escape(p)}</li>\" for p in info.positives) or \"<li>No specific positives</li>\"\n",
    "\n",
    "        output.append(\n",
    "            f\"<div class='card'>\"\n",
    "            f\"<header>{html.escape(topic)}</header>\"\n",
    "\n",
    "            f\"<section class='side'>\"\n",
    "            f\"<div class='stat'><span class='num'>{info.review_count}</span><span class='label'>Reviews</span></div>\"\n",
    "            f\"<div class='stat'><span class='num'>{info.avg_rating:.2f}/5 {info.stars}</span>\"\n",
    "            f\"<span class='label'>Average</span></div>\"\n",
    "            f\"<div class='stat'><span class='label'>Sentiment</span><div class='bar'>{bar}</div></div>\"\n",
    "            \"</section>\"\n",
    "\n",
    "            \"<section class='main'>\"\n",
    "            f\"<p class='summary'>{html.escape(info.summary)}</p>\"\n",
    "            \"<div class='cols'>\"\n",
    "            f\"<div><h4>Issues</h4><ul>{issues_html}</ul></div>\"\n",
    "            f\"<div><h4>Positives</h4><ul>{pos_html}</ul></div>\"\n",
    "            \"</div>\"\n",
    "            f\"<div class='chips'><h4>Keywords</h4>{kw_html}</div>\"\n",
    "            f\"<div class='chips'><h4>Phrases</h4>{ph_html}</div>\"\n",
    "            \"</section>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    output.append(\"</div>\")\n",
    "    display(HTML(\"\\n\".join(output)))\n",
    "\n",
    "def _bar_seg(name: str, frac: float) -> str:\n",
    "    cls = \"pos\" if name == \"positive\" else \"neg\" if name == \"negative\" else \"neu\"\n",
    "    pct = max(frac * 100, 1)\n",
    "    return f\"<span class='seg {cls}' style='width:{pct}%' title='{name}: {frac:.1%}'></span>\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  VISUAL HELPER – MATPLOTLIB CHART\n",
    "# -------------------------------------------------------------------\n",
    "def _plot_chart(data: Dict[str, TopicSummary], df: pd.DataFrame) -> None:\n",
    "    rows = [(t, d.avg_rating, d.review_count) for t, d in data.items() if t.lower() != \"other\"]\n",
    "    if not rows:\n",
    "        return\n",
    "    topics, ratings, counts = zip(*rows)\n",
    "    order = sorted(range(len(topics)), key=lambda i: ratings[i])\n",
    "    topics = [topics[i] for i in order]\n",
    "    ratings = [ratings[i] for i in order]\n",
    "    counts = [counts[i] for i in order]\n",
    "\n",
    "    cmap = mpl.colormaps.get_cmap(\"viridis\").resampled(len(topics))\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    bars = plt.barh(topics, ratings, color=[cmap(i) for i in range(len(topics))])\n",
    "\n",
    "    for bar, cnt, rating in zip(bars, counts, ratings):\n",
    "        x = rating + 0.05 if rating < 2.5 else rating - 0.6\n",
    "        clr = \"black\" if rating < 2.5 else \"white\"\n",
    "        plt.text(x, bar.get_y() + bar.get_height() / 2, f\"n={cnt}\", va=\"center\",\n",
    "                 color=clr, fontweight=\"bold\")\n",
    "\n",
    "    mean = df[\"score\"].mean()\n",
    "    plt.axvline(mean, linestyle=\"--\", linewidth=2, label=f\"Overall {mean:.2f}\")\n",
    "    plt.xticks(range(1, 6), [\"★\" * i for i in range(1, 6)])\n",
    "    plt.xlim(0, 5.2)\n",
    "    plt.xlabel(\"Star Rating\")\n",
    "    plt.ylabel(\"Topic\")\n",
    "    plt.title(\"BMW App – Average Rating by Topic\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  MISC HELPERS\n",
    "# -------------------------------------------------------------------\n",
    "def _setup_logger() -> logging.Logger:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "    return logging.getLogger(\"bmw_topic_cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple translation evaluation with qwen3:8b …\n",
      "Reading bmw_app_analysis/translations/bmw_reviews_sampledHE.csv …\n",
      "Detected delimiter ';'\n",
      "\n",
      "Sample data from first row:\n",
      "  content: Die App kann schon viel, aber warum lassen sich di...\n",
      "  language: German\n",
      "  Translation A: The app can already do a lot, but why can't the wi...\n",
      "  Translation B: The app can do a lot, but why can't the windows or...\n",
      "\n",
      "Evaluating 120 rows with qwen3:8b …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 0 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German text. The source is a complaint about an app not allowing cert...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [00:27<54:01, 27.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 1 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's take a look at these two translations. The source text is in German, and I need to compare each translation against the original. ...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/120 [00:53<51:59, 26.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 2 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. The source is \"Top App, verständlich, übersichtlich und...\n",
      "✅ Scores: A=5, B=4, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 3/120 [01:58<1:26:07, 44.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 3 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German text. First, I need to understand the original German sentence...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/120 [02:52<1:33:01, 48.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 4 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this translation scoring. First, I need to compare each translation to the source text. The source is in German, and there ...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/120 [03:43<1:34:07, 49.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 5 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German sentence. The source is \"Einfach eine sehr gute und hilfreiche...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/120 [03:59<1:12:14, 38.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 6 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. First, I need to understand the source text and then co...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7/120 [04:37<1:11:19, 37.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 7 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. First, I need to understand the original German text.\n",
      "\n",
      "...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8/120 [05:23<1:15:25, 40.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 8 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German sentence. The source is \"Alles wichtige ist schnell abrufbar.\"...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 9/120 [06:36<1:33:26, 50.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 9 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German text. The source is \"Funktioniert einwandfrei. Bin bis jetzt v...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10/120 [06:59<1:17:08, 42.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 10 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me look at these translations. The source text is in German. The user wants me to rate each translation against the original, not ag...\n",
      "✅ Scores: A=4, B=3, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/120 [08:00<1:27:04, 47.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 11 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German text. First, I need to understand the source text and then com...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 12/120 [08:27<1:14:49, 41.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 12 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. The source is about an app crashing on a new phone afte...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 13/120 [09:03<1:11:05, 39.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 13 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. The source is about missing info on fuel and mileage fo...\n",
      "✅ Scores: A=4, B=3, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 14/120 [09:24<1:00:28, 34.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 14 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. The source is \"schade dass der Concierge Service einges...\n",
      "✅ Scores: A=4, B=5, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 15/120 [09:58<59:48, 34.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 15 (German) ---\n",
      "Model output: <think>\n",
      "Okay, I need to rate these two translations of the German text. Let's start by understanding the source text. The German says, \"Die App hat im...\n",
      "✅ Scores: A=4, B=3, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 16/120 [10:20<52:57, 30.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 16 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German text. First, I need to understand the original German text and...\n",
      "✅ Scores: A=4, B=5, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 17/120 [11:19<1:06:57, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 17 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German sentence. The source is \"Remote Parking fehlt obwohl Fahrzeug ...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 18/120 [11:53<1:03:48, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 18 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's start by looking at the source text. The German says: \"Es fehlen einfache Features die heute eigentlich Standart sind: -Kein Dark ...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 19/120 [12:30<1:02:56, 37.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 19 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me try to figure out the scores for these two translations. First, I need to compare each translation against the original German te...\n",
      "✅ Scores: A=4, B=3, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20/120 [13:16<1:06:40, 40.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 20 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this translation scoring. The user wants me to rate two translations of a German text. Let me start by understanding the so...\n",
      "⚠️ Using default scores due to error: Could not extract scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 21/120 [14:42<1:28:27, 53.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 21 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of a German text. The source is about someone being happy with an app tha...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 22/120 [15:15<1:17:37, 47.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 22 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German text. The source is about updating a BMW vehicle's status thro...\n",
      "✅ Scores: A=5, B=1, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 23/120 [15:43<1:07:35, 41.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 23 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this translation scoring. First, I need to compare each translation against the original German text. Let me start with Tra...\n",
      "⚠️ Using default scores due to error: Could not extract scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 24/120 [17:18<1:32:01, 57.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 24 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of a German sentence. The source is \"Bis jetzt funktionierte alles einwand...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 25/120 [17:40<1:14:21, 46.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 25 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this translation scoring. The source text is in German: \"is von BMW...muss also gut sein ;) tolle App funktioniert und mac...\n",
      "✅ Scores: A=4, B=3, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 26/120 [18:08<1:04:50, 41.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 26 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this translation evaluation. The source text is German: \"Nach jedem Update wieder neu anmelden - genial.\" The user provided...\n",
      "✅ Scores: A=4, B=5, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 27/120 [18:36<57:41, 37.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 27 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this translation comparison. The source is in German: \"BMW ist besser als mein RangeRover. Vor allem was die digitale Techn...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 28/120 [19:09<55:25, 36.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 28 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. I need to rate the two translations of the German text. The source is \"Einfach genial wenn man stets in ein temperier...\n",
      "✅ Scores: A=4, B=5, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 29/120 [19:46<55:07, 36.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 29 (German) ---\n",
      "Model output: <think>\n",
      "Okay, let's take a look at these translations. The source text is in German, and I need to evaluate both translations against it. \n",
      "\n",
      "First, the...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 30/120 [20:25<55:29, 36.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 30 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, I need to rate these two translations of the Italian text. Let me start by understanding the source text thoroughly. The Italian text me...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 31/120 [21:19<1:02:42, 42.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 31 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. I need to rate the two translations of the Italian text based on the given scoring scale. Let me start by understandi...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 32/120 [22:09<1:05:21, 44.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 32 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of an Italian text. The source is in Italian, and there are two English t...\n",
      "✅ Scores: A=4, B=5, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 33/120 [22:57<1:06:09, 45.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 33 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle these translations. The source text is in Italian, and I need to compare the two English translations. Let me start by read...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 34/120 [23:48<1:07:45, 47.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 34 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The source text is \"Comandi da remoto verso la macchina molto lenti.\" First, I need to understand the Italian sentenc...\n",
      "✅ Scores: A=4, B=2, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 35/120 [24:03<53:16, 37.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 35 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of an Italian sentence. The source is \"Utile, peccato che a volte non fa i...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 36/120 [24:45<54:15, 38.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 36 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. I need to rate the two translations of the Italian text. First, I'll read the source text carefully to understand the...\n",
      "✅ Scores: A=4, B=3, Winner=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 37/120 [25:14<49:34, 35.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 37 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of an Italian sentence. The source is \"sarebbe molto utile connessione co...\n",
      "✅ Scores: A=3, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 38/120 [25:46<47:21, 34.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 38 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of an Italian text. The source is \"Digital key non compatibile con s22, c...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 39/120 [26:32<51:24, 38.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 39 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this translation comparison. The source is in Italian: \"decisamente un grande aiuto per avere sempre l'autovettura sotto co...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 40/120 [26:58<46:09, 34.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 40 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, I need to rate these two translations of the Italian text. Let me start by understanding the source text. The user is talking about the ...\n",
      "✅ Scores: A=4, B=4, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 41/120 [27:35<46:31, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 41 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let me look at these translations. The source text is in Italian: \"facile da usare, efficente e utile per avere il controllo dell'auto d...\n",
      "✅ Scores: A=5, B=5, Winner=Tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 42/120 [28:06<44:04, 33.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 42 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let's tackle this. The user wants me to rate two translations of an Italian text. The source is about a problem with trip recording that...\n",
      "✅ Scores: A=2, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 43/120 [28:37<42:26, 33.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 43 (Italian) ---\n",
      "Model output: <think>\n",
      "Okay, let me tackle this. The user wants me to rate two translations of an Italian text. The source is \"precisione e modalità di avviso sempre...\n",
      "✅ Scores: A=2, B=4, Winner=B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 44/120 [29:19<45:02, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating row 44 (Italian) ---\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Evaluate two candidate translations of app reviews by giving a simple \n",
    "numerical score (1-5) to each translation compared to the original.\n",
    "\n",
    "Key features\n",
    "----------------\n",
    "* Simple 1-5 scoring for each translation\n",
    "* Shows model output for each evaluation (truncated)\n",
    "* Automatic delimiter detection (handles ';', ',', or '\\t')\n",
    "* Better error handling with fallback scores\n",
    "* Summary statistics broken down by language\n",
    "\n",
    "Author: <your name>\n",
    "Date: 2025‑05‑04\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#                              Helper functions                               #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def detect_delimiter(file_path: str, sample_bytes: int = 2048) -> str:\n",
    "    \"\"\"Return the most likely delimiter in the CSV file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sample = f.read(sample_bytes)\n",
    "\n",
    "        # Quick manual checks\n",
    "        if \"\\t\" in sample:\n",
    "            return \"\\t\"\n",
    "        if \";\" in sample:\n",
    "            return \";\"\n",
    "\n",
    "        # Fallback to csv.Sniffer\n",
    "        try:\n",
    "            dialect = csv.Sniffer().sniff(sample)\n",
    "            return dialect.delimiter\n",
    "        except Exception:\n",
    "            return \",\"  # default\n",
    "\n",
    "\n",
    "def run_ollama(prompt: str, model_name: str, max_retries: int = 2) -> str:\n",
    "    \"\"\"Call `ollama run` with retries and a timeout; return raw stdout.\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            proc = subprocess.run(\n",
    "                [\"ollama\", \"run\", model_name],\n",
    "                input=prompt,\n",
    "                text=True,\n",
    "                capture_output=True,\n",
    "                timeout=120,  # Increased timeout to 120 seconds\n",
    "            )\n",
    "            return proc.stdout.strip()\n",
    "        except subprocess.TimeoutExpired:\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Timeout. Retrying {attempt + 1}/{max_retries} …\")\n",
    "                time.sleep(5)  # Longer cooling period between retries\n",
    "            else:\n",
    "                return \"ERROR: Timeout\"\n",
    "        except Exception as exc:\n",
    "            if attempt < max_retries:\n",
    "                print(f\"{exc}. Retrying {attempt + 1}/{max_retries} …\")\n",
    "                time.sleep(5)  # Longer cooling period between retries\n",
    "            else:\n",
    "                return f\"ERROR: {exc}\"\n",
    "\n",
    "\n",
    "def create_simple_prompt(row: pd.Series) -> str:\n",
    "    \"\"\"Build a simple scoring prompt with clear scale definition.\"\"\"\n",
    "    return f\"\"\"Rate these two English translations of this {row['language']} text:\n",
    "\n",
    "SOURCE TEXT ({row['language']}):\n",
    "{row['content']}\n",
    "\n",
    "TRANSLATION A:\n",
    "{row['Translation A']}\n",
    "\n",
    "TRANSLATION B:\n",
    "{row['Translation B']}\n",
    "\n",
    "SCORING SCALE:\n",
    "1 = Very poor (major errors, missing information, or incomprehensible)\n",
    "2 = Poor (significant errors or awkward phrasing)\n",
    "3 = Acceptable (gets the main meaning across with minor issues)\n",
    "4 = Good (fluent and accurate with very minor issues)\n",
    "5 = Excellent (perfect or near-perfect translation)\n",
    "\n",
    "IMPORTANT: Rate each translation against the source, not against each other.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Please respond with ONLY:\n",
    "score_A: [number]\n",
    "score_B: [number]\n",
    "\n",
    "No explanations or additional text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_score_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse the LLM response containing simple scores.\n",
    "    Returns a dict with scores and winner determination.\n",
    "    \"\"\"\n",
    "    if response.startswith(\"ERROR:\"):\n",
    "        return {\"error\": response, \"score_A\": None, \"score_B\": None, \"winner\": \"Error\"}\n",
    "    \n",
    "    # Look for simple \"score_A: X\" and \"score_B: Y\" patterns\n",
    "    score_a = None\n",
    "    score_b = None\n",
    "    \n",
    "    # Check for score_A pattern\n",
    "    a_match = re.search(r'score_?A:?\\s*(\\d+)', response, re.IGNORECASE)\n",
    "    if a_match:\n",
    "        try:\n",
    "            score_val = int(a_match.group(1))\n",
    "            if 1 <= score_val <= 5:\n",
    "                score_a = score_val\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Check for score_B pattern\n",
    "    b_match = re.search(r'score_?B:?\\s*(\\d+)', response, re.IGNORECASE)\n",
    "    if b_match:\n",
    "        try:\n",
    "            score_val = int(b_match.group(1))\n",
    "            if 1 <= score_val <= 5:\n",
    "                score_b = score_val\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # If we found both scores with regex\n",
    "    if score_a is not None and score_b is not None:\n",
    "        if score_a > score_b:\n",
    "            winner = \"A\"\n",
    "        elif score_b > score_a:\n",
    "            winner = \"B\"\n",
    "        else:\n",
    "            winner = \"Tie\"\n",
    "        return {\"score_A\": score_a, \"score_B\": score_b, \"winner\": winner}\n",
    "    \n",
    "    # Try to extract JSON from the response as fallback\n",
    "    try:\n",
    "        # Find the JSON object in the response\n",
    "        start_idx = response.find(\"{\")\n",
    "        end_idx = response.rfind(\"}\")\n",
    "        \n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response[start_idx:end_idx+1]\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            # Extract scores\n",
    "            score_a = data.get(\"score_A\")\n",
    "            score_b = data.get(\"score_B\")\n",
    "            \n",
    "            # Determine winner based on scores\n",
    "            if score_a is not None and score_b is not None:\n",
    "                if score_a > score_b:\n",
    "                    winner = \"A\"\n",
    "                elif score_b > score_a:\n",
    "                    winner = \"B\"\n",
    "                else:\n",
    "                    winner = \"Tie\"\n",
    "                \n",
    "                return {\"score_A\": score_a, \"score_B\": score_b, \"winner\": winner}\n",
    "            else:\n",
    "                return {\"error\": \"Missing scores in response\", \"winner\": \"Unclear\"}\n",
    "        else:\n",
    "            # No JSON found, but we already tried regex above\n",
    "            return {\"error\": \"Could not extract scores\", \"winner\": \"Unclear\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"winner\": \"Error\"}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#                             Main entry‑point                                #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def evaluate_translations(\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "    model_name: str = \"mistral:7b\",  # Changed default to faster model\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load review/translation CSV, evaluate each pair with simple scoring, save results.\"\"\"\n",
    "    print(f\"Reading {input_file} …\")\n",
    "    delimiter = detect_delimiter(input_file)\n",
    "    print(f\"Detected delimiter '{delimiter}'\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            sep=delimiter,\n",
    "            quotechar='\"',\n",
    "            doublequote=True,\n",
    "            engine=\"python\",  # forgiving\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(f\"❌ Could not load CSV: {exc}\")\n",
    "        return None\n",
    "\n",
    "    # Required columns\n",
    "    required = [\"content\", \"language\", \"Translation A\", \"Translation B\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"❌ Missing required columns:\", missing)\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        return None\n",
    "\n",
    "    # Display sample data to verify it loaded correctly\n",
    "    print(\"\\nSample data from first row:\")\n",
    "    for col in required:\n",
    "        val = df.iloc[0][col]\n",
    "        # Truncate for display\n",
    "        if isinstance(val, str) and len(val) > 50:\n",
    "            val = val[:50] + \"...\"\n",
    "        print(f\"  {col}: {val}\")\n",
    "\n",
    "    # Add score and judgment columns if not present\n",
    "    if \"score_A\" not in df.columns:\n",
    "        df[\"score_A\"] = pd.NA\n",
    "    if \"score_B\" not in df.columns:\n",
    "        df[\"score_B\"] = pd.NA\n",
    "    if \"judgment\" not in df.columns:\n",
    "        df[\"judgment\"] = pd.NA\n",
    "\n",
    "    # Evaluate rows\n",
    "    print(f\"\\nEvaluating {len(df)} rows with {model_name} …\")\n",
    "    \n",
    "    # Track success/error counts\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx in tqdm(df.index):\n",
    "        # Skip if already evaluated\n",
    "        if pd.notna(df.at[idx, \"score_A\"]) and pd.notna(df.at[idx, \"score_B\"]):\n",
    "            print(f\"Skipping row {idx} (already evaluated)\")\n",
    "            success_count += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Evaluating row {idx} ({df.at[idx, 'language']}) ---\")\n",
    "        \n",
    "        prompt = create_simple_prompt(df.loc[idx])\n",
    "        response = run_ollama(prompt, model_name)\n",
    "        \n",
    "        # Display truncated response\n",
    "        truncated_response = response[:150] + \"...\" if len(response) > 150 else response\n",
    "        print(f\"Model output: {truncated_response}\")\n",
    "\n",
    "        # Parse the response\n",
    "        result = parse_score_response(response)\n",
    "        \n",
    "        # Store the scores and judgment\n",
    "        if \"error\" in result and result.get(\"score_A\") is None:\n",
    "            # Apply default scores on error\n",
    "            df.at[idx, \"score_A\"] = 3  # Default neutral score\n",
    "            df.at[idx, \"score_B\"] = 3  # Default neutral score\n",
    "            df.at[idx, \"judgment\"] = \"Error\"\n",
    "            df.at[idx, \"error\"] = result[\"error\"]\n",
    "            print(f\"⚠️ Using default scores due to error: {result['error']}\")\n",
    "            error_count += 1\n",
    "            time.sleep(5)  # Longer cooling after error\n",
    "        else:\n",
    "            score_a = result.get(\"score_A\")\n",
    "            score_b = result.get(\"score_B\")\n",
    "            winner = result.get(\"winner\", \"Unclear\")\n",
    "            \n",
    "            df.at[idx, \"score_A\"] = score_a\n",
    "            df.at[idx, \"score_B\"] = score_b\n",
    "            df.at[idx, \"judgment\"] = winner\n",
    "            \n",
    "            print(f\"✅ Scores: A={score_a}, B={score_b}, Winner={winner}\")\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                df.at[idx, \"error\"] = result[\"error\"]\n",
    "                print(f\"⚠️ {result['error']} (but got scores)\")\n",
    "            \n",
    "            success_count += 1\n",
    "        \n",
    "        # Add cooling period between all calls\n",
    "        time.sleep(2)  # Prevent rate limiting\n",
    "\n",
    "    # Final save\n",
    "    try:\n",
    "        df.to_csv(output_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "        print(f\"✅ Results written to {output_file}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"⚠️ CSV save failed ({exc}); trying Excel …\")\n",
    "        try:\n",
    "            df.to_excel(output_file.replace(\".csv\", \".xlsx\"), index=False)\n",
    "            print(\"✅ Saved as Excel instead\")\n",
    "        except Exception as exc2:\n",
    "            print(f\"❌ Could not save results at all: {exc2}\")\n",
    "            return None\n",
    "\n",
    "    # Process success/error stats\n",
    "    print(f\"\\n📊 Processing stats: {success_count} successes, {error_count} errors\")\n",
    "\n",
    "    # -------------------- UPDATED SUMMARY SECTION --------------------\n",
    "    print(\"\\n====== EVALUATION SUMMARY ======\")\n",
    "    \n",
    "    # Overall summary\n",
    "    print(\"\\n🌐 OVERALL RESULTS:\")\n",
    "    counts = df[\"judgment\"].value_counts(dropna=False)\n",
    "    for label, cnt in counts.items():\n",
    "        pct = cnt / len(df) * 100\n",
    "        print(f\"  {label:8}: {cnt:>4}  ({pct:5.1f} %)\")\n",
    "    \n",
    "    a_avg = df[\"score_A\"].mean()\n",
    "    b_avg = df[\"score_B\"].mean()\n",
    "    diff = a_avg - b_avg\n",
    "    \n",
    "    print(f\"\\n  Average Scores (1-5):\")\n",
    "    print(f\"  Translation A: {a_avg:.2f}\")\n",
    "    print(f\"  Translation B: {b_avg:.2f}\")\n",
    "    print(f\"  Difference (A-B): {diff:+.2f}\")\n",
    "    \n",
    "    # Per-language statistics\n",
    "    print(\"\\n🔍 RESULTS BY LANGUAGE:\")\n",
    "    \n",
    "    # Create a list of languages to process in a specific order\n",
    "    languages = [\"German\", \"Italian\", \"French\", \"Spanish\"]\n",
    "    \n",
    "    # Ensure all expected languages are actually in the dataset\n",
    "    present_languages = df[\"language\"].unique()\n",
    "    print(f\"  Languages in dataset: {', '.join(sorted(present_languages))}\")\n",
    "    \n",
    "    # For each language, calculate stats\n",
    "    for lang in languages:\n",
    "        lang_df = df[df[\"language\"] == lang]\n",
    "        \n",
    "        if len(lang_df) == 0:\n",
    "            print(f\"\\n  ❌ No reviews for {lang}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  📊 {lang} ({len(lang_df)} reviews):\")\n",
    "        \n",
    "        # Judgment distribution\n",
    "        lang_counts = lang_df[\"judgment\"].value_counts(dropna=False)\n",
    "        for label in [\"A\", \"B\", \"Tie\", \"Error\", \"Unclear\"]:\n",
    "            if label in lang_counts:\n",
    "                cnt = lang_counts[label]\n",
    "                pct = cnt / len(lang_df) * 100\n",
    "                print(f\"    {label:8}: {cnt:>3}  ({pct:5.1f} %)\")\n",
    "        \n",
    "        # Scores\n",
    "        lang_a_avg = lang_df[\"score_A\"].mean()\n",
    "        lang_b_avg = lang_df[\"score_B\"].mean()\n",
    "        lang_diff = lang_a_avg - lang_b_avg\n",
    "        \n",
    "        print(f\"\\n    Scores (1-5):\")\n",
    "        print(f\"    Translation A: {lang_a_avg:.2f}\")\n",
    "        print(f\"    Translation B: {lang_b_avg:.2f}\")\n",
    "        print(f\"    Difference (A-B): {lang_diff:+.2f}\")\n",
    "    \n",
    "    # Distribution of score differences\n",
    "    print(\"\\n📈 SCORE DIFFERENCE DISTRIBUTION:\")\n",
    "    \n",
    "    # Calculate score differences\n",
    "    df[\"score_diff\"] = df[\"score_A\"] - df[\"score_B\"]\n",
    "    \n",
    "    # Group differences into categories\n",
    "    diff_categories = {\n",
    "        \"A much better (≥2)\": (df[\"score_diff\"] >= 2).sum(),\n",
    "        \"A better (1)\": (df[\"score_diff\"] == 1).sum(),\n",
    "        \"Tie (0)\": (df[\"score_diff\"] == 0).sum(),\n",
    "        \"B better (-1)\": (df[\"score_diff\"] == -1).sum(),\n",
    "        \"B much better (≤-2)\": (df[\"score_diff\"] <= -2).sum()\n",
    "    }\n",
    "    \n",
    "    for label, count in diff_categories.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {label:20}: {count:>3}  ({pct:5.1f} %)\")\n",
    "    \n",
    "    # Scale meaning reminder\n",
    "    print(\"\\n📋 SCORING SCALE REFERENCE:\")\n",
    "    print(\"  1 = Very poor (major errors, missing information, or incomprehensible)\")\n",
    "    print(\"  2 = Poor (significant errors or awkward phrasing)\")\n",
    "    print(\"  3 = Acceptable (gets the main meaning across with minor issues)\")\n",
    "    print(\"  4 = Good (fluent and accurate with very minor issues)\")\n",
    "    print(\"  5 = Excellent (perfect or near-perfect translation)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#                                 CLI hook                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    IN_FILE = \"bmw_app_analysis/translations/bmw_reviews_sampledHE.csv\"\n",
    "    OUT_FILE = \"bmw_app_analysis/translations/bmw_reviews_sampledLLM.csv\"\n",
    "    \n",
    "    # Changed to faster model options\n",
    "    MODEL = \"qwen3:8b\"  # alternative options: mistral:7b, phi3:mini, llama3:8b\n",
    "    \n",
    "    print(f\"Starting simple translation evaluation with {MODEL} …\")\n",
    "    evaluate_translations(IN_FILE, OUT_FILE, MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentiNext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
